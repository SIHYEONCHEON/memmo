import json
from fastapi import FastAPI, HTTPException,Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from ai_app.chatbot import Chatbot  # ê¸°ì¡´ ì½”ë“œì˜ Chatbot í´ë˜ìŠ¤ ì„í¬íŠ¸
from ai_app.chatbotStream import ChatbotStream
from ai_app.assist.common import client, model
from fastapi.responses import StreamingResponse
import asyncio
from ai_app.assist.characters import instruction,system_role
from ai_app.utils.function_calling import FunctionCalling, tools # ë‹¨ì¼ í•¨ìˆ˜ í˜¸ì¶œ
from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì‹œì‘ ì‹œì—ëŠ” ë³„ë‹¤ë¥¸ ë™ì‘ ì—†ì´ ë°”ë¡œ yield
    yield
    # ì¢…ë£Œ ì‹œ ì‹¤í–‰í•  ì½”ë“œ
    print("FastAPI shutting down...")
    chatbot.save_chat()
    print("Saved!")


app = FastAPI(lifespan=lifespan)
'''chatbot = Chatbot(
    model=model.basic,
    system_role = system_role,
    instruction=instruction,
    user= "ëŒ€ê¸°",
    assistant= "memmo"
    )  # ëª¨ë¸ ì´ˆê¸°í™”'''

chatbot=ChatbotStream(
    model=model.basic,
    system_role = system_role,
    instruction=instruction,
    user= "ëŒ€ê¸°",
    assistant= "memmo"
    )
# CORS ì„¤ì • (ì•ˆë“œë¡œì´ë“œ ì•± ì ‘ê·¼ í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)
''' FastAPIëŠ” Pydanticì„ ê¸°ë³¸ì ìœ¼ë¡œ ì§€ì›

FastAPIì—ì„œëŠ” ìš”ì²­ ë°ì´í„°ë¥¼ ë°›ì„ ë•Œ, request.jsonì„ ì§ì ‘ ì“°ì§€ ì•Šê³  Pydantic ëª¨ë¸ì„ ë§¤ê°œë³€ìˆ˜ë¡œ ë„£ìœ¼ë©´ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•´ ì¤˜ìš”.
-Flaskì—ì„œëŠ” ì§ì ‘ Pydanticì„ í˜¸ì¶œí•´ì•¼ í•¨
FlaskëŠ” ê¸°ë³¸ì ìœ¼ë¡œ Pydanticì„ ì§€ì›í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, request.jsonì„ ê°€ì ¸ì™€ì„œ Pydantic ëª¨ë¸ë¡œ ìˆ˜ë™ìœ¼ë¡œ ê²€ì¦í•´ì•¼ í•´ìš”.'''
class UserRequest(BaseModel):
    message: str

func_calling = FunctionCalling(model=model.basic)


@app.post("/stream-chat")
async def stream_chat(user_input: UserRequest):
    # 1) ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ìš°ì„  ì›ë³¸ ë¬¸ë§¥ì— ì¶”ê°€
    chatbot.add_user_message_in_context(user_input.message)
    
    chatbot.context[-1]['content'] += chatbot.instruction

    analyzed= func_calling.analyze(user_input.message, tools)

    temp_context = chatbot.to_openai_context().copy()
    

    for tool_call in analyzed:  # analyzedëŠ” list of function_call dicts
            if tool_call.type != "function_call":
                continue
            func_name = tool_call.name
            func_args = json.loads(tool_call.arguments)
            call_id = tool_call.call_id

            func_to_call = func_calling.available_functions.get(func_name)
            if not func_to_call:
                print(f"[ì˜¤ë¥˜] ë“±ë¡ë˜ì§€ ì•Šì€ í•¨ìˆ˜: {func_name}")
                continue

            try:
               
                function_call_msg = {
                    "type": "function_call",  # ê³ ì •
                    "call_id": call_id,  # ë”•ì…”ë„ˆë¦¬ ë‚´ì— ìˆê±°ë‚˜ keyê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ ì£¼ì˜
                    "name": func_name,
                    "arguments": tool_call.arguments  # dict -> JSON string
                }
                print(f"í•¨ìˆ˜ í˜¸ì¶œ ë©”ì‹œì§€: {function_call_msg}")
                if func_name == "search_internet":
                    # contextëŠ” ì´ë¯¸ run ë©”ì„œë“œì˜ ë§¤ê°œë³€ìˆ˜ë¡œ ë°›ê³  ìˆìŒ
                   func_response = func_to_call(chat_context=chatbot.context[:], **func_args)
                else:
                   func_response = func_to_call(**func_args)

                temp_context.extend([
                    function_call_msg,
                {
                    "type": "function_call_output",
                    "call_id": call_id,
                    "output": str(func_response)
                }
            ])
                print(temp_context)

            except Exception as e:
                print(f"[í•¨ìˆ˜ ì‹¤í–‰ ì˜¤ë¥˜] {func_name}: {e}")

    # 4) í•¨ìˆ˜ í˜¸ì¶œ ê²°ê³¼ê°€ ë°˜ì˜ëœ temp_contextìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìƒì„±
    async def generate_with_tool():
        try:
            # stream=Trueë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            stream = client.responses.create(
            model=chatbot.model,
            input=temp_context,  # user/assistant ì—­í•  í¬í•¨ëœ list êµ¬ì¡°
            top_p=1,
            stream=True,
            text={
                "format": {
                    "type": "text"  # ë˜ëŠ” "json_object" ë“± (Structured Output ì‚¬ìš© ì‹œ)
                }
            }
                )
              
            loading = True
            for event in stream:
                        match event.type:
                            case "response.created":
                                print("[ğŸ¤– ì‘ë‹µ ìƒì„± ì‹œì‘]")
                                loading = True
                                # ë¡œë”© ì• ë‹ˆë©”ì´ì…˜ìš© ëŒ€ê¸° ì‹œì‘
                                yield "â³ GPTê°€ ì‘ë‹µì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤..."
                                await asyncio.sleep(0)
                            case "response.output_text.delta":
                                if loading:
                                    print("\n[ğŸ’¬ ì‘ë‹µ ì‹œì‘ë¨ â†“]")

                                    loading = False
                                # ê¸€ì ë‹¨ìœ„ ì¶œë ¥
                                yield f"{event.delta}"
                                await asyncio.sleep(0)
                            

                            case "response.in_progress":
                                print("[ğŸŒ€ ì‘ë‹µ ìƒì„± ì¤‘...]")
                                yield "[ğŸŒ€ ì‘ë‹µ ìƒì„± ì¤‘...]"
                                yield "\n"

                            case "response.output_item.added":
                                if getattr(event.item, "type", None) == "reasoning":
                                    yield "[ğŸ§  GPTê°€ ì¶”ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤...]"
                                    yield "\n"
                                elif getattr(event.item, "type", None) == "message":
                                    yield "[ğŸ“© ë©”ì‹œì§€ ì•„ì´í…œ ì¶”ê°€ë¨]"
                                    yield "\n"
                            #ResponseOutputItemDoneEventëŠ” ìš°ë¦¬ê°€ case "response.output_item.done"ì—ì„œ ì¡ì•„ì•¼ í•´
                            case "response.output_item.done":
                                item = event.item
                                if item.type == "message" and item.role == "assistant":
                                    for part in item.content:
                                        if getattr(part, "type", None) == "output_text":
                                            completed_text= part.text
                            case "response.completed":
                                yield "\n"
                                #print(f"\nğŸ“¦ ìµœì¢… ì „ì²´ ì¶œë ¥: \n{completed_text}")
                            case "response.failed":
                                print("âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨")
                                yield "âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨"
                            case "error":
                                print("âš ï¸ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì—ëŸ¬ ë°œìƒ!")
                                yield "âš ï¸ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì—ëŸ¬ ë°œìƒ!"
                            case _:
                                yield "\n"
                                yield f"[ğŸ“¬ ê¸°íƒ€ ì´ë²¤íŠ¸ ê°ì§€: {event.type}]"
        except Exception as e:
            yield f"\nStream Error: {str(e)}"
        finally:
            # ìŠ¤íŠ¸ë¦¬ë°ì´ ëë‚˜ë©´ ìµœì¢… ì‘ë‹µì„ ì›ë³¸ ë¬¸ë§¥ì—ë§Œ ë°˜ì˜í•˜ê³  ì„ì‹œë¬¸ë§¥ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                            # ê¸°ì¡´ clean ë°©ì‹ ìœ ì§€
            chatbot.add_response_stream( completed_text)
            
                        # ìµœì¢… ì‘ë‹µì„ ì›ë³¸ ë¬¸ë§¥ì— ì €ì¥
    # 5) í•¨ìˆ˜ í˜¸ì¶œì´ ìˆì„ ë•ŒëŠ” ìœ„ì˜ generate_with_tool()ë¥¼ ì‚¬ìš©
    return StreamingResponse(generate_with_tool(), media_type="text/plain")






