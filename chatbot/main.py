import json
from fastapi import FastAPI, HTTPException,Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from ai_app.chatbot import Chatbot  # ê¸°ì¡´ ì½”ë“œì˜ Chatbot í´ë˜ìŠ¤ ì„í¬íŠ¸
from ai_app.chatbotStream import ChatbotStream
from ai_app.common import client, model
from fastapi.responses import StreamingResponse
import asyncio
from ai_app.characters import instruction,system_role
from ai_app.utils.function_calling import FunctionCalling, tools # ë‹¨ì¼ í•¨ìˆ˜ í˜¸ì¶œ
from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì‹œì‘ ì‹œì—ëŠ” ë³„ë‹¤ë¥¸ ë™ì‘ ì—†ì´ ë°”ë¡œ yield
    yield
    # ì¢…ë£Œ ì‹œ ì‹¤í–‰í•  ì½”ë“œ
    print("FastAPI shutting down...")
    chatbot.save_chat()
    print("Saved!")


app = FastAPI(lifespan=lifespan)
'''chatbot = Chatbot(
    model=model.basic,
    system_role = system_role,
    instruction=instruction,
    user= "ëŒ€ê¸°",
    assistant= "memmo"
    )  # ëª¨ë¸ ì´ˆê¸°í™”'''

chatbot=ChatbotStream(
    model=model.basic,
    system_role = system_role,
    instruction=instruction,
    user= "ëŒ€ê¸°",
    assistant= "memmo"
    )
# CORS ì„¤ì • (ì•ˆë“œë¡œì´ë“œ ì•± ì ‘ê·¼ í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)
''' FastAPIëŠ” Pydanticì„ ê¸°ë³¸ì ìœ¼ë¡œ ì§€ì›

FastAPIì—ì„œëŠ” ìš”ì²­ ë°ì´í„°ë¥¼ ë°›ì„ ë•Œ, request.jsonì„ ì§ì ‘ ì“°ì§€ ì•Šê³  Pydantic ëª¨ë¸ì„ ë§¤ê°œë³€ìˆ˜ë¡œ ë„£ìœ¼ë©´ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•´ ì¤˜ìš”.
-Flaskì—ì„œëŠ” ì§ì ‘ Pydanticì„ í˜¸ì¶œí•´ì•¼ í•¨
FlaskëŠ” ê¸°ë³¸ì ìœ¼ë¡œ Pydanticì„ ì§€ì›í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, request.jsonì„ ê°€ì ¸ì™€ì„œ Pydantic ëª¨ë¸ë¡œ ìˆ˜ë™ìœ¼ë¡œ ê²€ì¦í•´ì•¼ í•´ìš”.'''
class UserRequest(BaseModel):
    message: str

func_calling = FunctionCalling(model=model.basic)


@app.post("/stream-chat")
async def stream_chat(user_input: UserRequest):
    # 1) ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ìš°ì„  ì›ë³¸ ë¬¸ë§¥ì— ì¶”ê°€
    chatbot.add_user_message_in_context(user_input.message)
    
    # ì¶”ê°€ ì§€ì‹œì‚¬í•­(ê¸°ì¡´ ì½”ë“œì— ìˆë˜ ë¶€ë¶„)
    chatbot.context[-1]['content'] += chatbot.instruction

    # 2) ì‚¬ìš©ì ì…ë ¥ì„ ë¶„ì„í•´ í•¨ìˆ˜ í˜¸ì¶œì´ í•„ìš”í•œì§€ í™•ì¸
    # -- ì—¬ê¸°ì„œ ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„ --
    analyzed, analyzed_dict = func_calling.analyze(user_input.message, tools)

    # 3) í•¨ìˆ˜ í˜¸ì¶œ(íˆ´ ì‹¤í–‰)ì´ ìˆëŠ”ì§€ í™•ì¸
    if analyzed_dict.get("tool_calls"):
        # í•¨ìˆ˜ í˜¸ì¶œì´ ìˆë‹¤ë©´ ì„ì‹œë¬¸ë§¥ì„ ìƒì„±
        temp_context=chatbot.to_openai_context()[:]
        temp_context.append(analyzed)      # ë¶„ì„ëœ ë©”ì‹œì§€ë¥¼ ì„ì‹œë¬¸ë§¥ì— ì¶”ê°€
        tool_calls = analyzed_dict['tool_calls']

        for tool_call in tool_calls:
            function = tool_call["function"]
            func_name = function["name"]
            func_to_call = func_calling.available_functions[func_name]

            try:
                # í•¨ìˆ˜ ì¸ì íŒŒì‹±
                func_args = json.loads(function["arguments"])
                # ì‹¤ì œ í•¨ìˆ˜ í˜¸ì¶œ
                func_response = func_to_call(**func_args)
                temp_context.append({
                    "tool_call_id": tool_call["id"],
                    "role": "tool",
                    "name": func_name,
                    "content": str(func_response)
                })#ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¬¸ë§¥ì— ì¶”ê°€
            except Exception as e:
                # í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ì—ëŸ¬ ì²˜ë¦¬
                error_msg = f"[í•¨ìˆ˜ ì‹¤í–‰ ì˜¤ë¥˜] {str(e)}"
                # ì›í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë¡œê·¸ ë‚¨ê¸°ê±°ë‚˜ ìŠ¤íŠ¸ë¦¬ë° ë°˜í™˜ ê°€ëŠ¥
                temp_context.append({
                    "role": "assistant",
                    "content": error_msg
                })

        # 4) í•¨ìˆ˜ í˜¸ì¶œ ê²°ê³¼ê°€ ë°˜ì˜ëœ ì„ì‹œ ë¬¸ë§¥ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìƒì„±
        async def generate_with_tool():
            collected_text = ""
            try:
                # stream=Trueë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
                response = client.chat.completions.create(
                    model=chatbot.model,
                    messages=temp_context,
                    temperature=0.5,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0,
                    stream=True
                )
                for chunk in response:
                    if chunk.choices:
                        delta = chunk.choices[0].delta
                        if delta.content:
                            text_piece = delta.content
                            yield f"{text_piece}"
                            await asyncio.sleep(0)
                            collected_text += text_piece
            except Exception as e:
                yield f"\nStream Error: {str(e)}"
            finally:
                # ìŠ¤íŠ¸ë¦¬ë°ì´ ëë‚˜ë©´ ìµœì¢… ì‘ë‹µì„ ì›ë³¸ ë¬¸ë§¥ì—ë§Œ ë°˜ì˜í•˜ê³  ì„ì‹œë¬¸ë§¥ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                               # ê¸°ì¡´ clean ë°©ì‹ ìœ ì§€
                chatbot.add_response_stream(collected_text)
                
                          # ìµœì¢… ì‘ë‹µì„ ì›ë³¸ ë¬¸ë§¥ì— ì €ì¥
        # 5) í•¨ìˆ˜ í˜¸ì¶œì´ ìˆì„ ë•ŒëŠ” ìœ„ì˜ generate_with_tool()ë¥¼ ì‚¬ìš©
        return StreamingResponse(generate_with_tool(), media_type="text/plain")

    else:
        # í•¨ìˆ˜ í˜¸ì¶œì´ ì—†ëŠ” ê²½ìš°, ê¸°ì¡´ ë¡œì§ëŒ€ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
        async def generate():
            collected_text = ""
            try:
                response = client.chat.completions.create(
                    model=chatbot.model,
                    messages=chatbot.to_openai_context(),
                    temperature=0.5,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0,
                    stream=True
                )
                for chunk in response:
                    if chunk.choices:
                        delta = chunk.choices[0].delta
                        if delta.content:
                            text_piece = delta.content
                            yield f"{text_piece}"
                            await asyncio.sleep(0)
                            collected_text += text_piece
            except Exception as e:
                yield f"\nStream Error: {str(e)}"
            finally:
                
                chatbot.add_response_stream(collected_text)
                chatbot.clean_context()
                print(chatbot.context)

        # í•¨ìˆ˜ í˜¸ì¶œì´ ì—†ì„ ë•ŒëŠ” ê¸°ì¡´ generate() ì‚¬ìš©
        return StreamingResponse(generate(), media_type="text/plain")

@app.post("/completion-chat")
async def chat_api(request_data: UserRequest):
    ''' FastAPIì˜ request_dataëŠ” Pydantic ëª¨ë¸ ê°ì²´ë¼ì„œ .(ì )ìœ¼ë¡œ ì ‘ê·¼ ê°€ëŠ¥
FastAPIì—ì„œëŠ” ìš”ì²­ ë°ì´í„°ë¥¼ ë°›ì„ ë•Œ, Pydantic ëª¨ë¸(UserRequest)ì„ ì‚¬ìš©í•´ JSON ë°ì´í„°ë¥¼ Python ê°ì²´ë¡œ ë³€í™˜í•´ ì¤˜ìš”.
ì¦‰, request_dataëŠ” ê·¸ëƒ¥ dictê°€ ì•„ë‹ˆë¼ í´ë˜ìŠ¤ ê°ì²´ì²˜ëŸ¼ ë™ì‘í•˜ëŠ” Pydantic ì¸ìŠ¤í„´ìŠ¤ê°€ ë¼ìš”!

ğŸ’¡ FastAPIì—ì„œëŠ” request_dataê°€ Pydantic ëª¨ë¸ ê°ì²´ì´ë¯€ë¡œ request_data.request_messageì²˜ëŸ¼ ë©¤ë²„ ë³€ìˆ˜ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆìŒ.'''
    request_message = request_data.request_message
    print("request_message:", request_message)
    chatbot.add_user_message_in_context(request_message)

    
    # ì±—GPTì—ê²Œ í•¨ìˆ˜ì‚¬ì–‘ì„ í† ëŒ€ë¡œ ì‚¬ìš©ì ë©”ì‹œì§€ì— í˜¸ì‘í•˜ëŠ” í•¨ìˆ˜ ì •ë³´ë¥¼ ë¶„ì„í•´ë‹¬ë¼ê³  ìš”ì²­
    analyzed_dict=func_calling.analyze(request_message,tools)
    # ì±—GPTê°€ í•¨ìˆ˜ í˜¸ì¶œì´ í•„ìš”í•˜ë‹¤ê³  ë¶„ì„í–ˆëŠ”ì§€ ì—¬ë¶€ ì²´í¬
    if analyzed_dict.get("function_call"): # ë‹¨ì¼ í•¨ìˆ˜ í˜¸ì¶œ
        response = func_calling.run( analyzed_dict, chatbot.context[:]) # ë‹¨ì¼ í•¨ìˆ˜ í˜¸ì¶œ
        chatbot.add_response(response)
    else:
        response = chatbot.send_request()#instructoinì¶”ê°€
        chatbot.add_response(response)

    response_message = chatbot.get_response()
    chatbot.clean_context()#instructoinì œê±°
    print("response_message:", response_message)
    return {"response_message": response_message}



