import os
import uuid
from pinecone import Pinecone
from openai import OpenAI
from ai_app.assist.common import client, model, embedding_model, today
from db.db_manager import get_mongo_collection

class AutoSummary:
    """
    - ëŒ€í™” ë©”ì‹œì§€ ìˆ˜(summarize_threshold) ì´ìƒ ëª¨ì´ë©´ ìš”ì•½ â†’ MongoDB ì €ì¥
    - ìš”ì•½ë¬¸(vectorize_threshold) ì´ìƒ ëˆ„ì ë˜ë©´ ë²¡í„°í™” â†’ Pinecone ì €ì¥ â†’ MongoDB ìƒíƒœ ì—…ë°ì´íŠ¸
    - ì‚¬ìš©ìê°€ ì§€ë‚œ ëŒ€í™” ê²€ìƒ‰ ìš”ì²­ ì‹œ ë²¡í„°DB ì¡°íšŒ â†’ MongoDBì—ì„œ ì›ë¬¸ ë°˜í™˜ (search_memory)
    """
    def __init__(
        self,
        summarize_threshold: int = 10,
        summary_length: int = 100,
        mongo_db: str = "memmo",
        mongo_col: str = "summaries",
        pinecone_index_name: str = "memmo-ai"
    ):
        self.summarize_threshold = summarize_threshold
        self.summary_length = summary_length
        self.summary_col = get_mongo_collection(mongo_db, mongo_col)
        pc = Pinecone(os.getenv("PINECONE_API_KEY"))
        self.pinecone_index = pc.Index(pinecone_index_name)
        self.embed_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    def classify_query_type(self, user_query: str) -> str:
        # âœ… ê¸°ì¡´ 2ë¶„ë¥˜ì—ì„œ 3ë¶„ë¥˜ë¡œ í™•ì¥ë¨ (memory_search / internet_search / context)
        prompt = f'''
        ë‹¤ìŒ ì‚¬ìš©ì ì…ë ¥ì˜ ìœ í˜•ì„ íŒë‹¨í•˜ì‹œì˜¤. ì•„ë˜ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µí•˜ì‹œì˜¤:

        - "memory_search": ê³¼ê±° ëŒ€í™” ë‚´ìš©(ì˜ˆ: ì´ì „ì— ë§í•œ ë…¼ë¬¸ ì£¼ì œ, ë‚´ê°€ ì „ì— ì–˜ê¸°í•œ ëª©ì  ë“±)ì„ íšŒìƒí•˜ë ¤ëŠ” ì§ˆë¬¸
        - "internet_search": ì‹¤ì‹œê°„ ì •ë³´ë‚˜ ì™¸ë¶€ ì„¸ê³„ì˜ ì‚¬ì‹¤(ì˜ˆ: ì˜¤ëŠ˜ ë‚ ì”¨, ì„œìš¸ëŒ€ ìœ„ì¹˜, í™˜ìœ¨ ë“±)ì„ ë¬»ëŠ” ì§ˆë¬¸
        - "context": ê·¸ ì™¸ ì¼ë°˜ì ì¸ ë¬¸ë§¥ ê¸°ë°˜ ëŒ€í™”

        ì§ˆë¬¸: "{user_query}"
        '''
        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}]
            )
            classification = response.choices[0].message.content.strip().lower()
            return classification
        except Exception as e:
            print(f"[AutoSummary] classify_query_type ì˜¤ë¥˜: {e}")
            return "context"

    def maybe_summarize(self, context: list[dict]):
        unsaved = [m for m in context if not m.get("saved", False)]
        if len(unsaved) < self.summarize_threshold:
            return

        system_msg = {
            "role": "system",
            "content": (
                f"ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ë˜, ì•„ë˜ í˜•ì‹ì— ë”°ë¼ í•­ëª©ë³„ë¡œ êµ¬ì¡°í™”í•˜ì—¬ ì‘ì„±í•˜ì‹­ì‹œì˜¤.\n\n"
                f"ì¶œë ¥ í˜•ì‹:\n"
                f"- ì‚¬ìš©ì ì´ë¦„: (ì˜ˆ: ì´ìƒìœ¤)\n"
                f"- ì‘ì„± ëª©ì : (ì´ ì‚¬ìš©ìê°€ ê¸€ì„ ì“°ë ¤ëŠ” ì´ìœ  ë˜ëŠ” ë™ê¸°)\n"
                f"- ì¡°ê±´ ë° ìŠ¤íƒ€ì¼: (í˜•ì‹, ë¶„ëŸ‰, ë§íˆ¬ ë“± ìš”ì²­ëœ ì œì•½ ì‚¬í•­)\n"
                f"- ì£¼ìš” ëŒ€í™” ìš”ì•½:\n"
                f"  - ì‚¬ìš©ì: ...\n"
                f"  - assistant: ...\n\n"
                f"[ì£¼ì˜ ì‚¬í•­]\n"
                f"- ë°˜ë“œì‹œ ëª¨ë“  í•­ëª©ì„ í¬í•¨í•˜ì‹­ì‹œì˜¤. ë‹¨, í•­ëª© ë‚´ìš©ì´ ì—†ì„ ê²½ìš° 'ì—†ìŒ'ì´ë¼ê³  ëª…ì‹œí•˜ì‹­ì‹œì˜¤.\n"
                f"- ëŒ€í™” ë„ì¤‘ ì‚¬ìš©ìì˜ ëª©ì ì´ë‚˜ ìŠ¤íƒ€ì¼ì´ ë³€ê²½ë˜ë©´ 'ì‘ì„± ëª©ì ' ë˜ëŠ” 'ì¡°ê±´ ë° ìŠ¤íƒ€ì¼' í•­ëª©ì— ë°˜ì˜í•˜ì‹­ì‹œì˜¤.\n"
                f"- í•­ëª©ë³„ ê¸¸ì´ëŠ” ê°„ê²°í•˜ë˜ í•µì‹¬ì€ ë¹ ëœ¨ë¦¬ì§€ ë§ˆì‹­ì‹œì˜¤.\n"
                f"- ì „ì²´ ìš”ì•½ ê¸¸ì´ëŠ” {self.summary_length}ì ë‚´ì™¸ë¡œ í•˜ë˜, ì¤‘ìš”í•œ ì •ë³´ê°€ ë§ìœ¼ë©´ ì¡°ê¸ˆ ë” ì¨ë„ ì¢‹ë‹¤"
            )
        }
        messages = [system_msg] + [{"role": m["role"], "content": m["content"]} for m in unsaved]
        resp = client.responses.create(model=model.advanced, input=messages)
        summary_text = resp.output_text.strip()

        print(f"[AutoSummary] ìš”ì•½ ì™„ë£Œ: {summary_text}")

        doc_id = str(uuid.uuid4())
        doc = {"_id": doc_id, "date": today(), "summary": summary_text, "vectorized": True}
        self.summary_col.insert_one(doc)

        for m in unsaved:
            m["saved"] = True
        for m in unsaved:
            if m in context:
                context.remove(m)

        context[:] = [msg for msg in context if not (msg["role"] == "system" and msg["content"].startswith("ì´ì „ ëŒ€í™” ìš”ì•½:"))]
        context.append({
            "role": "system",
            "content": f"ì´ì „ ëŒ€í™” ìš”ì•½: {summary_text}",
            "saved": True
        })

        self.vectorize_single(doc_id, summary_text, doc["date"])

    def vectorize_single(self, doc_id: str, summary_text: str, date: str):
        try:
            vec = self.embed_client.embeddings.create(
                input=summary_text,
                model=embedding_model.ada
            ).data[0].embedding

            self.pinecone_index.upsert(vectors=[{
                "id": doc_id,
                "values": vec,
                "metadata": {"date": date}
            }])

            print(f"[AutoSummary] ë²¡í„°DB ì—…ë¡œë“œ ì™„ë£Œ: ë¬¸ì„œ ID = {doc_id}")

        except Exception as e:
            print(f"[AutoSummary] ë²¡í„°í™” ì‹¤íŒ¨: {e}")

    def search_memory(self, query_text: str, top_k: int = 5) -> list[str]:
        try:
            embed_result = self.embed_client.embeddings.create(
                input=query_text,
                model=embedding_model.ada
            )
            query_vector = embed_result.data[0].embedding

            search_results = self.pinecone_index.query(
                vector=query_vector,
                top_k=top_k,
                include_metadata=True
            )

            summaries = []
            for match in search_results["matches"]:
                summary_doc = self.summary_col.find_one({"_id": match["id"]})
                if summary_doc:
                    summaries.append(summary_doc["summary"])

            print(f"[AutoSummary] ğŸ” ê²€ìƒ‰ëœ ìœ ì‚¬ ìš”ì•½ ê°œìˆ˜: {len(summaries)}")
            return summaries

        except Exception as e:
            print(f"[AutoSummary] search_memory ì˜¤ë¥˜: {e}")
            return []

    def answer_with_memory_check(self, user_query: str, context: list[dict]) -> list[str]:
        query_type = self.classify_query_type(user_query)
        print(f"[AutoSummary] ì§ˆì˜ ë¶„ë¥˜ ê²°ê³¼: {query_type}")
        

        # âœ… ìƒˆë¡œ ì¶”ê°€ëœ ë¶„ê¸°: ì¸í„°ë„· ê²€ìƒ‰ ìš”ì²­ì´ë©´ fallback ìì²´ë¥¼ ìƒëµ
        if query_type == "internet_search":
            print("[AutoSummary] â›” ì¸í„°ë„· ê²€ìƒ‰ ì§ˆë¬¸ìœ¼ë¡œ ë¶„ë¥˜ë¨ â†’ memory ì‘ë‹µ ì¤‘ë‹¨")
            return None

        # âœ… memory_searchë¡œ ë¶„ë¥˜ë˜ë”ë¼ë„ ìµœê·¼ 3ê°œì˜ ëŒ€í™”ë¬¸(context)ì— ì§ˆë¬¸ê³¼ ë™ì¼í•œ ë‚´ìš©ì´ ìˆìœ¼ë©´
        # â†’ ë¶ˆí•„ìš”í•œ ë²¡í„° fallbackì„ ìƒëµí•˜ê³  context ì‘ë‹µì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•¨.
        # â†’ ëª©ì : ë°©ê¸ˆ í•œ ì§ˆë¬¸ì— ëŒ€í•´ "ê¸°ì–µ ëª»í•¨" íšŒí”¼ê°€ ë°œìƒí•˜ì§€ ì•Šë„ë¡ ë³´ì™„.
        if query_type == "memory_search" and len(context) > 0:
            recent_texts = [m["content"] for m in context[-3:] if "content" in m]
            recent_combined = " ".join(recent_texts)

            # GPT ì‘ë‹µ í™•ì¸
            if user_query.strip() in recent_combined:
                messages = context + [{"role": "user", "content": user_query}]
                base_response = client.chat.completions.create(
                    model=model.advanced,
                    messages=messages
                ).choices[0].message.content.strip()

                # íšŒí”¼ì„± ì‘ë‹µì´ë©´ fallback ì‹¤í–‰
                if any(kw in base_response for kw in ["ëª¨ë¦„", "ê¸°ì–µ", "ì ‘ê·¼í•  ìˆ˜ ì—†", "ì •ë³´ ì—†ìŒ"]):
                    print("[AutoSummary] âš  ë™ì¼ ë¬¸ì¥ ìˆì§€ë§Œ íšŒí”¼ ì‘ë‹µ â†’ fallback ê³„ì† ì§„í–‰")
                else:
                    print("[AutoSummary] âœ… ë™ì¼ ë¬¸ì¥ ìˆê³  ì •ìƒ ì‘ë‹µ â†’ fallback ìƒëµ")
                    return None
        
        messages = context + [{"role": "user", "content": user_query}]
        try:
            base_response = client.chat.completions.create(
                model=model.advanced,
                messages=messages
            ).choices[0].message.content.strip()
        except Exception as e:
            print(f"[AutoSummary] GPT ê¸°ë³¸ ì‘ë‹µ ì‹¤íŒ¨: {e}")
            return ["ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."]

        fallback_check_prompt = f'''
        ì•„ë˜ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ GPT ì‘ë‹µì„ ë³´ê³ ,
        GPTê°€ ì§ˆë¬¸ì— ëª…í™•íˆ ë‹µí•˜ì§€ ëª»í–ˆê±°ë‚˜,
        'ê¸°ì–µí•˜ì§€ ëª»í•¨', 'ëª¨ë¦„', 'ì •ë³´ ì—†ìŒ', 'ê°œì¸ì •ë³´ ë¶ˆê°€' ë“±ì˜ í‘œí˜„ìœ¼ë¡œ íšŒí”¼í•œ ê²½ìš°ì—ëŠ” "FALLBACK",
        ê·¸ ì™¸ ì •ìƒì ì¸ ì •ë³´ ì „ë‹¬ì´ í¬í•¨ëœ ì‘ë‹µì´ë©´ "OK"ë¼ê³ ë§Œ ë‹µí•˜ì‹œì˜¤.

        [ì§ˆë¬¸]: {user_query}
        [ì‘ë‹µ]: {base_response}
        '''
        try:
            fallback_classification = client.chat.completions.create(
                model=model.advanced,
                messages=[{"role": "user", "content": fallback_check_prompt}]
            ).choices[0].message.content.strip().upper()
        except Exception as e:
            print(f"[AutoSummary] fallback íŒë‹¨ ì˜¤ë¥˜: {e}")
            fallback_classification = "OK"

        memory_summaries = self.search_memory(user_query)

        # âœ… keyword_triggered ì œê±°ë¨ (ì˜¤íƒ ë°©ì§€ ëª©ì )
        should_fallback = (
            query_type != "internet_search" and (
            query_type == "memory_search"
            or fallback_classification == "FALLBACK"
            )
        )
        
        print(f"[AutoSummary] ğŸ§  fallback íŒë‹¨ ê·¼ê±° â†’ ì§ˆì˜ìœ í˜•: '{query_type}', ì‘ë‹µíŒë‹¨: '{fallback_classification}'") #05-22 fallbackì´ ì–´ë–¤ ì¡°ê±´ì— ì˜í•´ íŠ¸ë¦¬ê±°ë˜ì—ˆëŠ”ê°€? ì¶”ê°€ 

        if should_fallback:
            print("[AutoSummary] âœ… fallback ì¡°ê±´ ì¶©ì¡± â†’ memory ì‘ë‹µ ìˆ˜í–‰")
            memory_summaries = self.search_memory(user_query)

            if not memory_summaries:
                print("[AutoSummary] âš  memory summary ì—†ìŒ â†’ context ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„±")
                return [base_response]
            
            print(f"[AutoSummary] âœ… memory summary {len(memory_summaries)}ê°œ ì‚¬ìš© â†’ memory ê¸°ë°˜ ì‘ë‹µ ìƒì„±")

            memory_context = [
                {
                    "role": "system",
                    "content": (
                        "ë‹¤ìŒ ì§ˆë¬¸ì€ ì•„ë˜ ìš”ì•½ ë‚´ìš©ì„ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬ ì‘ë‹µí•˜ì‹œì˜¤. \n\n"
                        f"ì´ì „ ëŒ€í™” ìš”ì•½: {s}"
                    )
                }
                for s in memory_summaries
            ]

            try:
                fallback_response = client.chat.completions.create(
                    model=model.advanced,
                    messages=memory_context + context + [{"role": "user", "content": user_query}]
                ).choices[0].message.content.strip()
                return [fallback_response]
            except Exception as e:
                print(f"[AutoSummary] GPT fallback ì‘ë‹µ ì‹¤íŒ¨: {e}")
                return ["ì£„ì†¡í•©ë‹ˆë‹¤. íšŒìƒ ì‘ë‹µë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."]

        print("[AutoSummary] âš  fallback ì¡°ê±´ ë¯¸ì¶©ì¡± â†’ context ì‘ë‹µ ì‚¬ìš©")
        return [base_response]

from fastapi import APIRouter
from pydantic import BaseModel

router = APIRouter()

class SearchRequest(BaseModel):
    query_text: str
    top_k: int = 5

_auto_summary_instance: AutoSummary | None = None

def get_auto_summary() -> AutoSummary:
    global _auto_summary_instance
    if _auto_summary_instance is None:
        _auto_summary_instance = AutoSummary()
    return _auto_summary_instance

@router.post("/memory/search")
async def search_memory_endpoint(req: SearchRequest):
    auto_summary = get_auto_summary()
    results = auto_summary.search_memory(req.query_text, req.top_k)
    return {"results": results}
