import os
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))
import json
from ai_app.assist.common import client, model,makeup_response
from ai_app.assist.characters import instruction,system_role
import math
from ai_app.utils.function_calling import FunctionCalling,tools
from db.memory_manager import MemoryManager 
from ai_app.assist.ConversationContextFactory import ConversationContextFactory
from ai_app.assist.ConversationContextFactory import ContextDict 
from ai_app.utils.writingRequirementsManager import WritingRequirementsManager
from ai_app.assist.characters import get_update_field_prompt
from typing import List, TypedDict, Literal
from ai_app.utils.auto_summary import AutoSummary

class MessageDict(TypedDict):
    role: Literal["user", "assistant"]
    content: str
    saved: bool
class ChatbotStream:
    def __init__(self, model,system_role,instruction,**kwargs):
        """
        ì´ˆê¸°í™”:
          - context ë¦¬ìŠ¤íŠ¸ ìƒì„± ë° ì‹œìŠ¤í…œ ì—­í•  ì„¤ì •
          - sub_contexts ì„œë¸Œ ëŒ€í™”ë°© ë¬¸ë§¥ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ {í•„ë“œì´ë¦„,ë¬¸ë§¥,ìš”ì•½,ì§ˆë¬¸} êµ¬ì„±
          - current_field = í˜„ì¬ ëŒ€í™”ë°© ì¶”ì  (ê¸°ë³¸ê°’: ë©”ì¸ ëŒ€í™”ë°©
          - openai.api_key ì„¤ì •
          - ì‚¬ìš©í•  ëª¨ë¸ëª… ì €ì¥
          - ì‚¬ìš©ì ì´ë¦„
          - assistant ì´ë¦„ë¦„
        """
        self.context = [{"role": "system","content": system_role}]
        # ì„œë¸Œ ëŒ€í™”ë°© ë¬¸ë§¥ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
        #í˜„ì¬ ëŒ€í™” ë§¥ë½ì„ ì¸ì§€,(í•„ë“œëŒ€í™”ëƒ ë©”ì¸ëŒ€í™”ëƒ=> ì¦‰ ì±—ë´‡í´ë˜ìŠ¤ ì¬í™œìš©)
        self.sub_contexts :dict[str, ContextDict] = {}        
        self.current_field = "main"
        
        self.model = model
        self.instruction=instruction

        self.max_token_size = 16 * 1024 #ìµœëŒ€ í† í°ì´ìƒì„ ì“°ë©´ ì˜¤ë¥˜ê°€ë°œìƒ ë”°ë¼ì„œ í† í° ìš©ëŸ‰ê´€ë¦¬ê°€ í•„ìš”.
        self.available_token_rate = 0.9#ìµœëŒ€í† í°ì˜ 90%ë§Œ ì“°ê² ë‹¤.
    
        self.username=kwargs["user"]
        self.assistantname=kwargs["assistant"]
        self.memoryManager = MemoryManager()
        self.writingRequirementsManager=WritingRequirementsManager()
              # â† AutoSummary ì´ˆê¸°í™”: ë©”ì‹œì§€ 10íšŒë§ˆë‹¤ ìš”ì•½ê³¼ ë™ì‹œì— ë²¡í„°í™”
        self.auto_summary = AutoSummary(
            summarize_threshold=10,
            summary_length=100
        )
        self.field_instructions = {
            "purpose_background": "ë‹¹ì‹ ì˜ ì—­í• ì€ ê¸€ì„ ì“°ëŠ” ì´ìœ ì™€ ë°°ê²½ì„ ëª…í™•íˆ ì •ë¦¬í•˜ëŠ” ì—­í• ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ ë‹µí•˜ë©´ì„œ ì‚¬íšŒì„±ë†’ì€ ì…œë¡ë‹µê²Œ ì‚¬ìš©ìì—ê²Œ ë‹¹ì‹ ì´ í•„ìš”í•œ ì •ë³´ë¥¼ ë¬¼ì–´ë³´ê±°ë‚˜ ëŒ€ë‹µí•˜ì„¸ìš”.ì‚¬ìš©ìì˜ ì˜¤íƒ€ì—ëŠ” ì–¸ê¸‰í•˜ì§€ë§ê³  ë‹µí•˜ì„¸ìš”",
            "context_topic": "ê¸€ì˜ ì£¼ì œë‚˜ ìƒí™©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì •ë¦¬í•˜ëŠ” ì—­í• ì…ë‹ˆë‹¤.ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì‚¬íšŒì„±ë†’ì€ ì…œë¡ë‹µê²Œ ì‚¬ìš©ìì—ê²Œ ë‹¹ì‹ ì´ í•„ìš”í•œ ì •ë³´ë¥¼ ë¬¼ì–´ë³´ê±°ë‚˜ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš” ì‚¬ìš©ìì˜ ì˜¤íƒ€ì—ëŠ” ì–¸ê¸‰í•˜ì§€ë§ê³  ë‹µí•˜ì„¸ìš”",
            "audience_scope": "ëŒ€ìƒ ë…ìì˜ íŠ¹ì„±ê³¼ ëª©ì ì— ë§ê²Œ ì •ë¦¬í•˜ëŠ” ì—­í• ì…ë‹ˆë‹¤.ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì‚¬íšŒì„±ë†’ì€ ì…œë¡ë‹µê²Œ ì‚¬ìš©ìì—ê²Œ ë‹¹ì‹ ì´ í•„ìš”í•œ ì •ë³´ë¥¼ ë¬¼ì–´ë³´ê±°ë‚˜ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš” ì‚¬ìš©ìì˜ ì˜¤íƒ€ì—ëŠ” ì–¸ê¸‰í•˜ì§€ë§ê³  ë‹µí•˜ì„¸ìš”",
            "format_structure": "ê¸€ì˜ êµ¬ì¡°ë‚˜ í˜•ì‹ì„ ë…¼ë¦¬ì  ìˆœì„œë¡œ ì •ë¦¬í•˜ëŠ” ì—­í• ì…ë‹ˆë‹¤.ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì‚¬íšŒì„±ë†’ì€ ì…œë¡ë‹µê²Œ ì‚¬ìš©ìì—ê²Œ ë‹¹ì‹ ì´ í•„ìš”í•œ ì •ë³´ë¥¼ ë¬¼ì–´ë³´ê±°ë‚˜ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš” ì‚¬ìš©ìì˜ ì˜¤íƒ€ì—ëŠ” ì–¸ê¸‰í•˜ì§€ë§ê³  ë‹µí•˜ì„¸ìš”",
            "logic_evidence": "ë…¼ë¦¬ ì „ê°œë‚˜ ê·¼ê±°, ìë£Œê°€ ì˜ ë“œëŸ¬ë‚˜ë„ë¡ ì •ë¦¬í•˜ëŠ” ì—­í• ì…ë‹ˆë‹¤.ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì‚¬íšŒì„±ë†’ì€ ì…œë¡ë‹µê²Œ ì‚¬ìš©ìì—ê²Œ ë‹¹ì‹ ì´ í•„ìš”í•œ ì •ë³´ë¥¼ ë¬¼ì–´ë³´ê±°ë‚˜ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš” ì‚¬ìš©ìì˜ ì˜¤íƒ€ì—ëŠ” ì–¸ê¸‰í•˜ì§€ë§ê³  ë‹µí•˜ì„¸ìš”",
            "expression_method": "ë¬¸ì²´, ì–´ì¡°, ì‹œì  ë“±ì„ ì¼ê´€ë˜ê²Œ ì •ë¦¬í•˜ëŠ” ì—­í• ì…ë‹ˆë‹¤.ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì‚¬íšŒì„±ë†’ì€ ì…œë¡ë‹µê²Œ ì‚¬ìš©ìì—ê²Œ ë‹¹ì‹ ì´ í•„ìš”í•œ ì •ë³´ë¥¼ ë¬¼ì–´ë³´ê±°ë‚˜ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš” ì‚¬ìš©ìì˜ ì˜¤íƒ€ì—ëŠ” ì–¸ê¸‰í•˜ì§€ë§ê³  ë‹µí•˜ì„¸ìš”",
            "additional_constraints": "í‚¤ì›Œë“œ, ê¸ˆì§€ì–´, ì¡°ê±´ ë“±ì˜ ì œì•½ì‚¬í•­ì„ ëª…í™•íˆ ì •ë¦¬í•˜ëŠ” ì—­í• ì…ë‹ˆë‹¤.ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì‚¬íšŒì„±ë†’ì€ ì…œë¡ë‹µê²Œ ì‚¬ìš©ìì—ê²Œ ë‹¹ì‹ ì´ í•„ìš”í•œ ì •ë³´ë¥¼ ë¬¼ì–´ë³´ê±°ë‚˜ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš” ì‚¬ìš©ìì˜ ì˜¤íƒ€ì—ëŠ” ì–¸ê¸‰í•˜ì§€ë§ê³  ë‹µí•˜ì„¸ìš”",
            "output_expectations": "ê²°ê³¼ë¬¼ í˜•íƒœë‚˜ ì™„ì„± ê¸°ì¤€ì„ êµ¬ì²´ì ìœ¼ë¡œ ì •ë¦¬í•˜ëŠ” ì—­í• ì…ë‹ˆë‹¤.ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì‚¬íšŒì„±ë†’ì€ ì…œë¡ë‹µê²Œ ì‚¬ìš©ìì—ê²Œ ë‹¹ì‹ ì´ í•„ìš”í•œ ì •ë³´ë¥¼ ë¬¼ì–´ë³´ê±°ë‚˜ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš” ì‚¬ìš©ìì˜ ì˜¤íƒ€ì—ëŠ” ì–¸ê¸‰í•˜ì§€ë§ê³  ë‹µí•˜ì„¸ìš”"
        }
       
    def add_user_message_in_context(self, message: str):
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€:
          - ì‚¬ìš©ìê°€ ì…ë ¥í•œ messageë¥¼ contextì— user ì—­í• ë¡œ ì¶”ê°€
        """
        assistant_message = {
            "role": "user",
            "content": message,
            "saved": False
        }
        if self.current_field == "main":
            self.context.append(assistant_message)
        else:
            self.sub_contexts[self.current_field]["messages"].append(assistant_message)
    #ì „ì†¡ë¶€
    def _send_request_Stream(self,temp_context=None):
        
        completed_text = ""

        if temp_context is None:
           current_context = self.get_current_context()
           openai_context = self.to_openai_context(current_context)
           stream = client.responses.create(
            model=self.model,
            input=openai_context,  
            top_p=1,
            stream=True,
            
            text={
                "format": {
                    "type": "text"  # ë˜ëŠ” "json_object" ë“± (Structured Output ì‚¬ìš© ì‹œ)
                }
            }
                )
        else:  
           stream = client.responses.create(
            model=self.model,
            input=temp_context,  # user/assistant ì—­í•  í¬í•¨ëœ list êµ¬ì¡°
            top_p=1,
            stream=True,
            text={
                "format": {
                    "type": "text"  # ë˜ëŠ” "json_object" ë“± (Structured Output ì‚¬ìš© ì‹œ)
                }
            }
                )
        
        loading = True  # deltaê°€ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ ë¡œë”© ì¤‘ ìƒíƒœ ìœ ì§€       
        for event in stream:
            #print(f"event: {event}")
            match event.type:
                case "response.created":
                    print("[ğŸ¤– ì‘ë‹µ ìƒì„± ì‹œì‘]")
                    loading = True
                    # ë¡œë”© ì• ë‹ˆë©”ì´ì…˜ìš© ëŒ€ê¸° ì‹œì‘
                    print("â³ GPTê°€ ì‘ë‹µì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤...")
                    
                case "response.output_text.delta":
                    if loading:
                        print("\n[ğŸ’¬ ì‘ë‹µ ì‹œì‘ë¨ â†“]")
                        loading = False
                    # ê¸€ì ë‹¨ìœ„ ì¶œë ¥
                    print(event.delta, end="", flush=True)
                 

                case "response.in_progress":
                    print("[ğŸŒ€ ì‘ë‹µ ìƒì„± ì¤‘...]")

                case "response.output_item.added":
                    if getattr(event.item, "type", None) == "reasoning":
                        print("[ğŸ§  GPTê°€ ì¶”ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤...]")
                    elif getattr(event.item, "type", None) == "message":
                        print("[ğŸ“© ë©”ì‹œì§€ ì•„ì´í…œ ì¶”ê°€ë¨]")
                #ResponseOutputItemDoneEventëŠ” ìš°ë¦¬ê°€ case "response.output_item.done"ì—ì„œ ì¡ì•„ì•¼ í•´
                case "response.output_item.done":
                    item = event.item
                    if item.type == "message" and item.role == "assistant":
                        for part in item.content:
                            if getattr(part, "type", None) == "output_text":
                                completed_text= part.text
                case "response.completed":
                    print("\n")
                    #print(f"\nğŸ“¦ ìµœì¢… ì „ì²´ ì¶œë ¥: \n{completed_text}")
                case "response.failed":
                    print("âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨")
                case "error":
                    print("âš ï¸ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì—ëŸ¬ ë°œìƒ!")
                case _:
                    
                    print(f"[ğŸ“¬ ê¸°íƒ€ ì´ë²¤íŠ¸ ê°ì§€: {event.type}]")
        return completed_text
  
    def send_request_Stream(self):
      self.context[-1]['content']+=self.instruction
      #contextë¬¸ ë§¨ ë§ˆì§€ë§‰ì— instructionì„ ì¶”ê°€í•´ë¼.
      return self._send_request_Stream()#->ì‹¤ì œ ë³´ë‚´ëŠ” ì½”ë“œëŠ” _send_request ì´ë‹¤,
#ì±—ë´‡ì— ë§ê²Œ ë¬¸ë§¥ íŒŒì‹±
    def add_response(self, response):
        response_message = {
            "role" : response['choices'][0]['message']["role"],
            "content" : response['choices'][0]['message']["content"],
            "saved" : False
        }
        self.context.append(response_message)

    def add_response_stream(self, response):
            """
    ì±—ë´‡ ì‘ë‹µì„ í˜„ì¬ ëŒ€í™”ë°©ì˜ ë¬¸ë§¥ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    
    Args:
        response (str): ì±—ë´‡ì´ ìƒì„±í•œ ì‘ë‹µ í…ìŠ¤íŠ¸.
    """
            assistant_message = {
            "role": "assistant",
            "content": response,
            "saved": False
        }
            if self.current_field == "main":
                # ë©”ì¸ ëŒ€í™”ë°© ë¬¸ë§¥ì— ì¶”ê°€
                self.context.append(assistant_message)
            else:
                if self.current_field not in self.sub_contexts:
                    self.sub_contexts[self.current_field] = {"messages": []}
                self.sub_contexts[self.current_field]["messages"].append(assistant_message)
      
            # â† ë©”ì‹œì§€ê°€ ë“¤ì–´ì˜¬ ë•Œë§ˆë‹¤ ìš”ì•½Â·ë²¡í„°í™” ê²€ì‚¬ ì¶”ê°€
            self.auto_summary.maybe_summarize(self.context)                    

    def get_response(self, response_text: str):
        """
        ì‘ë‹µë‚´ìš©ë°˜í™˜:
          - ë©”ì‹œì§€ë¥¼ ì½˜ì†”(ë˜ëŠ” UI) ì¶œë ¥ í›„, ê·¸ëŒ€ë¡œ ë°˜í™˜
        """
        print(response_text['choices'][0]['message']['content'])
        return response_text
#ë§ˆì§€ë§‰ ì§€ì¹¨ì œê±°ê±°
    def clean_context(self):
        '''
        1.contextë¦¬ìŠ¤íŠ¸ì— ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ë¶€í„° ì²˜ìŒê¹Œì§€ ìˆœíšŒí•œë‹¤
        2."instruction:\n"ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ìì—´ì„ ë‚˜ëˆˆë‹¤..ì²«userì„ ì°¾ìœ¼ë©´ ì•„ë˜ ê³¼ì •ì„ ì§„í–‰í•œë‹¤,
        3.ì²« ë²ˆì§¸ ë¶€ë¶„ [0]ë§Œ ê°€ì ¸ì˜¨ë‹¤. (ì¦‰, "instruction:\n" ì´ì „ì˜ ë¬¸ìì—´ë§Œ ë‚¨ê¸´ë‹¤.)
        4.strip()ì„ ì ìš©í•˜ì—¬ ì•ë’¤ì˜ ê³µë°±ì´ë‚˜ ê°œí–‰ ë¬¸ìë¥¼ ì œê±°í•œë‹¤.
        '''
        for idx in reversed(range(len(self.context))):
            if self.context[idx]['role']=='user':
                self.context[idx]["content"]=self.context[idx]['content'].split('instruction:\n')[0].strip()
                break
#ì§ˆì˜ì‘ë‹µ í† í° ê´€ë¦¬
    def handle_token_limit(self, response):
        # ëˆ„ì  í† í° ìˆ˜ê°€ ì„ê³„ì ì„ ë„˜ì§€ ì•Šë„ë¡ ì œì–´í•œë‹¤.
        try:
            current_usage_rate = response['usage']['total_tokens'] / self.max_token_size
            exceeded_token_rate = current_usage_rate - self.available_token_rate
            if exceeded_token_rate > 0:
                remove_size = math.ceil(len(self.context) / 10)
                self.context = [self.context[0]] + self.context[remove_size+1:]
        except Exception as e:
            print(f"handle_token_limit exception:{e}")
#apiìš”ì†Œì—ë§Œ í•´ë‹¹í•˜ëŠ”ë¶€ë¶„ë§Œ ë°˜í™˜í•´ ë¬¸ë§¥êµ¬ì„±ì„±
    def to_openai_context(self, context):
        return [{"role":v["role"], "content":v["content"]} for v in context]
    def get_current_context(self):
        if self.current_field == "main":
            return self.context
        else:
            return self.sub_contexts.get(self.current_field, {}).get("messages", [])
#dbì €ì¥ ë©”ì†Œë“œ
    def save_chat(self):
        self.memoryManager.save_chat(self.context)   
 #@[í•„ë“œ ëŒ€í™” ê´€ë¦¬]@

    def enter_sub_conversation(self, field_name: str) -> str:
        '''
        í˜„ì¬ ë“¤ì–´ê°„ í•„ë“œ ëŒ€í™” ì§„ì… ì²˜ë¦¬
        1.ê¸°ì¡´ í•„ë“œë°©ì´ ì—†ë‹¤ë©´ ë§Œë“ ë‹¤
        2.í˜„ì¬ subë¬¸ë§¥ì— ì§„ì…í•œ í•„ë“œë°©ì„ ì¶”ê°€í•œë‹¤
        3.í˜„ì¬ ì§„ì…í•œ í•„ë“œì˜ ì´ë¦„ì„ ë°”ê¾¼ë‹¤.
        4.ì§„ì…ë©”ì„¸ì§€ë¥¼ ì‚¬ìš©ìì—ê²Œ ì•Œë¦°ë‹¤.
        '''
        if field_name not in self.sub_contexts:
            self.sub_contexts[field_name] = ConversationContextFactory.create_context(field_name)
        self.current_field = field_name
        return f"{field_name} ì— ëŒ€í•´ ë„ì™€ë“œë¦´ê²Œìš”.ì–´ë–¤ ê±¸ ë„ì™€ ë“œë¦´ê¹Œìš”?"
    
    def exit_sub_conversation(self) -> str:
        '''ë°©ë‚˜ê°ˆë–„ ì²˜ë¦¬ë¡œì§
        1.í˜„ì¬ ì„œë¸ŒëŒ€í™” ë‚´ìš©ì„ ìš”ì•½ í›„ í•„ë“œ ë‚´ìš© ì—…ë°ì´íŠ¸
        2.í•„ë“œ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ì—ˆë‹¤ëŠ” ê²ƒë§Œ ë©”ì¸ë¬¸ë§¥ì— ì¶”ê°€
        3.ë°˜í™˜ê°’ì€ ì—…ë°ì´íŠ¸í•„ë“œì˜ ë¦¬í„´ìœ¼ë¡œë¡œ'''
        if self.current_field == "main":
            return "ì´ë¯¸ ë©”ì¸ ëŒ€í™”ë°©ì— ìˆìŠµë‹ˆë‹¤."
        # í˜„ì¬ ì„œë¸Œ ëŒ€í™”ë°© ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        field_name = self.current_field
        sub_context = self.sub_contexts[field_name]
        #ë¬¸ë§¥ëŒ€í™” ê°€ì ¸ì˜´
        conversation_text = " ".join([msg["content"] for msg in sub_context["messages"]])
        #í˜„ì¬ ëŒ€í™”ë‚´ìš© ìš”ì•½
        try:
            response = client.responses.create(
                        model=model.advanced, 
                        input=[{
                        "role": "user", 
                        "content": f"{conversation_text}\nì˜ ëŒ€í™”ë‚´ìš©ì„ ì •ë¦¬í•´ë¼"
                    }],
                    )
            summarized_content = response.output_text # ìš”ì•½ëœ ë‚´ìš© ì¶”ì¶œ
        except Exception as e:
                summarized_content = "ìš”ì•½ ì‹¤íŒ¨: ì›ë³¸ ëŒ€í™” ë‚´ìš© ìœ ì§€"
                print(f"ì—ëŸ¬ ë°œìƒ: {e}")
        # update_field í˜¸ì¶œë¡œ í•„ë“œ ì—…ë°ì´íŠ¸ ë° ìš”ì•½
        update_message = self.writingRequirementsManager.update_field(field_name, summarized_content)
        # ë©”ì¸ ë¬¸ë§¥ì— ê°„ë‹¨í•œ ë©”ì‹œì§€ ì¶”ê°€
        summary_message = f"í•„ë“œ '{field_name}'ì—ì„œ ëŒ€í™”ë¥¼ ë‚˜ëˆ”"
        if summary_message:
            print(f"[ëŒ€í™” ë‚´ìš© ìš”ì•½]: {summary_message}")
        else:
            print("[ëŒ€í™” ë‚´ìš© ìš”ì•½ì´ ì—†ìŠµë‹ˆë‹¤.]")
        self.add_response_stream(summary_message)
        # ë©”ì¸ ëŒ€í™”ë°©ìœ¼ë¡œ ì „í™˜
        self.current_field = "main"
        # update_fieldì—ì„œ ë°˜í™˜ëœ ë©”ì‹œì§€ë¥¼ ì‚¬ìš©ìì—ê²Œ ì „ë‹¬
        return update_message
    
    def add_user_message_in_context(self, message: str):
        user_message = {
            "role": "user",
            "content": message,
            "saved": False  # ì¶”í›„ ì €ì¥ ì—¬ë¶€ í™•ì¸ ì‹œ ì‚¬ìš©
        }

        if self.current_field == "main":
            self.context.append(user_message)
        else:
            current_messages = self.get_current_context()
            current_messages.append(user_message)
    
    def get_current_context(self) -> List[MessageDict]:
        """
        í˜„ì¬ í™œì„±í™”ëœ ëŒ€í™”ë°©ì˜ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        - ë©”ì¸ ëŒ€í™”ë°©ì´ë©´ self.context
        - ì„œë¸Œ ëŒ€í™”ë°©ì´ë©´ sub_contexts[field_name]["messages"]
        - ì„œë¸Œ ë°©ì´ ì•„ì§ ì—†ë‹¤ë©´ ì¦‰ì‹œ ìƒì„±
        """
        if self.current_field == "main":
            return self.context
        if self.current_field not in self.sub_contexts:
            self.sub_contexts[self.current_field] = ConversationContextFactory.create_context(self.current_field)
            '''ë§Œì•½ ì‚¬ìš©ìê°€ ë°©ì„ ëª…ì‹œì ìœ¼ë¡œ enter_sub_conversation() í•˜ì§€ ì•Šê³ ë„, ë°”ë¡œ ë©”ì‹œì§€ë¥¼ ë³´ë‚´ëŠ” ê²½ìš°:
            add_user_message_in_context()ë‚˜ get_current_context() í˜¸ì¶œ ì‹œ sub_contexts[field_name]ì´ ì—†ì„ ìˆ˜ ìˆìŒ. ì´ë•Œ ìë™ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” ë¹„ìƒìš© ì•ˆì „ ë¡œì§'''
        return self.sub_contexts[self.current_field]["messages"]


if __name__ == "__main__":
    '''ì‹¤í–‰íë¦„
    ë‹¨ê³„	ë‚´ìš©
1ï¸âƒ£	ì‚¬ìš©ì ì…ë ¥ ë°›ìŒ (user_input)
2ï¸âƒ£	â†’ add_user_message_in_context() ë¡œ user ë©”ì‹œì§€ë¥¼ ë¬¸ë§¥ì— ì¶”ê°€
3ï¸âƒ£	â†’ analyze() ë¡œ í•¨ìˆ˜ í˜¸ì¶œì´ í•„ìš”í•œì§€ íŒë‹¨
4ï¸âƒ£	â†’ í•„ìš”í•˜ë©´ í•¨ìˆ˜ ì‹¤í–‰ + ê²°ê³¼ë¥¼ temp_contextì— ì¶”ê°€
5ï¸âƒ£	â†’ chatbot._send_request_Stream(temp_context) ë¡œ ì‘ë‹µ ë°›ìŒ
6ï¸âƒ£	âœ… streamed_response ê²°ê³¼ë¥¼ ì§ì ‘ add_response_stream()ìœ¼ë¡œ ìˆ˜ë™ ì €ì¥'''
    
    chatbot = ChatbotStream(
        model.advanced,
        system_role=system_role,
        instruction=instruction,
        user="ëŒ€ê¸°",
        assistant="memmo")
    func_calling=FunctionCalling(model.advanced)
    print("===== Chatbot Started =====")
    print("ì´ˆê¸° context:", chatbot.context)
    print("ì‚¬ìš©ìê°€ 'exit'ë¼ê³  ì…ë ¥í•˜ë©´ ì¢…ë£Œí•©ë‹ˆë‹¤.\n")
    
    print(chatbot.sub_contexts)      # ì¶œë ¥: {}
    print(chatbot.current_field)

    while True:
        user_input = input("User > ")
        if user_input.strip().lower() == "exit":
            print("Chatbot ì¢…ë£Œ.")
            

        # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë¬¸ë§¥ì— ì¶”ê°€
        chatbot.add_user_message_in_context(user_input)

        # ì‚¬ìš©ì ì…ë ¥ ë¶„ì„ (í•¨ìˆ˜ í˜¸ì¶œ ì—¬ë¶€ í™•ì¸)
        analyzed = func_calling.analyze(user_input, tools)

        temp_context = chatbot.to_openai_context().copy()
        


        for tool_call in analyzed:  # analyzedëŠ” list of function_call dicts
            if tool_call.type != "function_call":
                continue
            
            func_name = tool_call.name
            func_args = json.loads(tool_call.arguments)
            call_id = tool_call.call_id

            func_to_call = func_calling.available_functions.get(func_name)
            if not func_to_call:
                print(f"[ì˜¤ë¥˜] ë“±ë¡ë˜ì§€ ì•Šì€ í•¨ìˆ˜: {func_name}")
                continue


            try:
               
                function_call_msg = {
                    "type": "function_call",  # ê³ ì •
                    "call_id": call_id,  # ë”•ì…”ë„ˆë¦¬ ë‚´ì— ìˆê±°ë‚˜ keyê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ ì£¼ì˜
                    "name": func_name,
                    "arguments": tool_call.arguments  # dict -> JSON string
                }
                print(f"í•¨ìˆ˜ í˜¸ì¶œ ë©”ì‹œì§€: {function_call_msg}")
                if func_name == "search_internet":
                    # contextëŠ” ì´ë¯¸ run ë©”ì„œë“œì˜ ë§¤ê°œë³€ìˆ˜ë¡œ ë°›ê³  ìˆìŒ
                    func_response = func_to_call(chat_context=chatbot.context[:], **func_args)
                else:
                    func_response=func_to_call(**func_args)
                

                temp_context.extend([
                    function_call_msg,
                {
                    "type": "function_call_output",
                    "call_id": call_id,
                    "output": str(func_response)
                }
            ])
              #  print("í•¨ìˆ˜ ì‹¤í–‰í›„ ì„ì‹œë¬¸ë§¥:{}".format(temp_context))

            except Exception as e:
                print(f"[í•¨ìˆ˜ ì‹¤í–‰ ì˜¤ë¥˜] {func_name}: {e}")

        # í•¨ìˆ˜ ê²°ê³¼ í¬í•¨ ì‘ë‹µ ìš”ì²­
        streamed_response = chatbot._send_request_Stream(temp_context=temp_context)
        temp_context = None
        chatbot.add_response_stream(streamed_response)
        print(chatbot.context)

    # === ë¶„ê¸° ì²˜ë¦¬ ë ===

    